import json
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, List

import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
from datasets import Dataset
from sklearn.model_selection import train_test_split
from transformers import (
    AutoModelForSequenceClassification,
    AutoTokenizer,
    DataCollatorWithPadding,
    Trainer,
    TrainingArguments,
)


@dataclass
class DistillationDataCollator(DataCollatorWithPadding):
    # ìƒì†ì„ í†µí•´ tokenizer.padë¥¼ ë‚´ë¶€ì ìœ¼ë¡œ ì•ˆì „í•˜ê²Œ ì‚¬ìš©í•©ë‹ˆë‹¤.

    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:
        # 1. soft_labelsë¥¼ ì•ˆì „í•˜ê²Œ ì¶”ì¶œ (ì´ë¯¸ í…ì„œì¼ ê°€ëŠ¥ì„±ì´ ë†’ìœ¼ë¯€ë¡œ stack ì‚¬ìš©)
        soft_labels = torch.stack([f.pop("soft_labels") for f in features])

        # 2. ë¶€ëª¨ í´ë˜ìŠ¤ì˜ íŒ¨ë”© ì²˜ë¦¬
        batch = super().__call__(features)

        # 3. float32 í˜•ë³€í™˜ í™•ì¸ í›„ ì‚½ì…
        batch["soft_labels"] = soft_labels.to(torch.float32)

        return batch


# 1. ì§€ì‹ ì¦ë¥˜ë¥¼ ìœ„í•œ ì»¤ìŠ¤í…€ íŠ¸ë ˆì´ë„ˆ ì •ì˜
class DistillationTrainer(Trainer):
    def __init__(self, *args, alpha=0.5, temperature=2.0, **kwargs):
        super().__init__(*args, **kwargs)
        self.alpha = alpha  # Soft Label(Teacher)ê³¼ Hard Label(ì •ë‹µ)ì˜ ë°˜ì˜ ë¹„ìœ¨
        self.temperature = temperature  # í™•ë¥  ë¶„í¬ë¥¼ ë¶€ë“œëŸ½ê²Œ ë§Œë“œëŠ” ê°€ì¤‘ì¹˜

    def compute_loss(
        self, model, inputs, return_outputs=False, num_items_in_batch=None
    ):
        # Teacherê°€ ì¤€ soft_labels ì¶”ì¶œ
        soft_labels = inputs.pop("soft_labels")
        labels = inputs.get("labels")

        # Student ëª¨ë¸ì˜ ì˜ˆì¸¡ (Logits)
        outputs = model(**inputs)
        student_logits = outputs.get("logits")

        # 1) Soft Loss: Teacherì˜ ë¶„í¬ì™€ Studentì˜ ë¶„í¬ ì°¨ì´ (KL Divergence)
        # Temperatureë¥¼ ì ìš©í•´ ë¶„í¬ë¥¼ ì™„ë§Œí•˜ê²Œ ë§Œë“­ë‹ˆë‹¤.
        soft_loss = nn.KLDivLoss(reduction="batchmean")(
            F.log_softmax(student_logits / self.temperature, dim=-1),
            F.softmax(soft_labels / self.temperature, dim=-1),
        ) * (self.temperature**2)

        # 2) Hard Loss: ì‹¤ì œ ì •ë‹µê³¼ì˜ ì°¨ì´ (Cross Entropy)
        hard_loss = F.cross_entropy(student_logits, labels)

        # ìµœì¢… ì†ì‹¤: ë‘ ë¡œìŠ¤ë¥¼ ì„ìŒ
        loss = self.alpha * soft_loss + (1.0 - self.alpha) * hard_loss

        return (loss, outputs) if return_outputs else loss


def train_bert():
    current_file = Path(__file__).resolve()
    root_dir = current_file.parents[2]
    src_dir = current_file.parents[1]

    # ë°ì´í„° ê²½ë¡œë¥¼ ì§€ì‹ ì¦ë¥˜ìš© íŒŒì¼ë¡œ ë³€ê²½
    data_path = root_dir / "train_data" / "distillation_data.json"
    model_path = src_dir / "models" / "base_model"
    output_dir = src_dir / "models" / "trained_bert"

    with open(data_path, "r", encoding="utf-8") as f:
        data = json.load(f)

    df = pd.DataFrame(data)

    # 2. ë°ì´í„° ì „ì²˜ë¦¬: soft_labels ë¦¬ìŠ¤íŠ¸ë¥¼ í…ì„œë¡œ ë³€í™˜í•  ì¤€ë¹„
    train_df, val_df = train_test_split(
        df, test_size=0.1, random_state=42, stratify=df["label"]
    )

    tokenizer = AutoTokenizer.from_pretrained(model_path)

    def tokenize_function(examples):
        result = tokenizer(
            examples["text"], truncation=True, padding="max_length", max_length=128
        )
        # Teacherê°€ ì¤€ í™•ë¥  ë¶„í¬ ì €ì¥
        result["soft_labels"] = examples["soft_labels"]
        return result

    train_dataset = Dataset.from_pandas(train_df, preserve_index=False).map(
        tokenize_function, batched=True
    )
    val_dataset = Dataset.from_pandas(val_df, preserve_index=False).map(
        tokenize_function, batched=True
    )

    # í…ì„œ í˜•ì‹ ì§€ì •
    train_dataset.set_format(
        type="torch", columns=["input_ids", "attention_mask", "label", "soft_labels"]
    )
    val_dataset.set_format(
        type="torch", columns=["input_ids", "attention_mask", "label", "soft_labels"]
    )

    model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=7)

    training_args = TrainingArguments(
        output_dir="./results",
        num_train_epochs=10,  # ì¦ë¥˜ í•™ìŠµì€ ì¡°ê¸ˆ ë” ì˜¤ë˜ í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤
        per_device_train_batch_size=16,
        learning_rate=5e-5,
        logging_steps=10,
        eval_strategy="epoch",
        save_strategy="epoch",
        load_best_model_at_end=True,
        remove_unused_columns=False,
    )

    # 3. ì»¤ìŠ¤í…€ ì¦ë¥˜ íŠ¸ë ˆì´ë„ˆ ì‚¬ìš©
    trainer = DistillationTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        data_collator=DistillationDataCollator(tokenizer=tokenizer),
        alpha=0.7,  # Teacherì˜ ë§ì„ 70% ë°˜ì˜
        temperature=5.0,  # ì§€ì‹ ì „ì´ë¥¼ ìœ„í•´ í™•ë¥  ë¶„í¬ë¥¼ ë” ë¶€ë“œëŸ½ê²Œ ì„¤ì •
    )

    print("ğŸš€ ì§€ì‹ ì¦ë¥˜ ê¸°ë°˜ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    trainer.train()

    model.save_pretrained(output_dir)
    tokenizer.save_pretrained(output_dir)
    print(f"âœ¨ ì¦ë¥˜ëœ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {output_dir}")


if __name__ == "__main__":
    train_bert()
